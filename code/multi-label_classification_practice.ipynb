{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e18141-aa90-46fc-8c5b-28e15ac4599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification, make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3379651-0e78-4fc1-8c39-04d3be584d61",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> X, y1 = make_classification(n_samples=10000, n_features=100, n_informative=30, n_classes=3, random_state=1)\n",
    ">>> y2 = shuffle(y1, random_state=1)\n",
    ">>> y3 = shuffle(y1, random_state=2)\n",
    ">>> Y = np.vstack((y1, y2, y3)).T\n",
    ">>> n_samples, n_features = X.shape # 10,100\n",
    ">>> n_outputs = Y.shape[1] # 3\n",
    ">>> n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49f46f2f-23c6-4525-b892-8200695f69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c9342b7-9d27-49cb-a7a7-6bd031b39c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=1)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "multi_target_forest.fit(X_train, y_train)\n",
    "y_pred = multi_target_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66a022e0-7a14-4316-86c9-546853e0ec77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0864"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_target_forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f266c-5cd0-4767-9977-97cff9497978",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6537ebe-683f-41a7-bff7-b4cc4f0ac7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp get_paramsMLPClassifier(random_state=1, max_iter=300)\n",
    "multi_target_nn = MultiOutputClassifier(mlp, n_jobs=-1)\n",
    "multi_target_nn.fit(X_train, y_train)\n",
    "y_pred = multi_target_nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b826154-4b00-4034-921d-607c406cc212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1056"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_target_nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bad17de-17ce-42c2-b36c-0fe22b7a9c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 300,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67a082-6d8f-4512-a1c4-50b551178ff6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c729824c-ecb2-4618-935e-e4526bdb6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import joblib\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0617060-75a0-47a4-8603-473a632cadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message, stem='lemm'):\n",
    "    \"\"\"Text processing.\n",
    "    \n",
    "    Args:\n",
    "        stem(str): stem or lemm.\n",
    "        \n",
    "    Returns:\n",
    "        list: Cleaned tokens.\n",
    "    \"\"\"\n",
    "    # 1. Cleaning\n",
    "\n",
    "    # 2. Normalization\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", message.lower())\n",
    "\n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Stop Word Removal\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tokens = list(filter(lambda w: w not in stop_words, tokens))\n",
    "\n",
    "    # 5. Part of Speech Tagging / Named Entity Recognition\n",
    "\n",
    "    # 6. Stemming or Lemmatization\n",
    "    # Because the targets are not roots, we should use Lemmatization\n",
    "\n",
    "    clean_tokens = []\n",
    "    if stem == 'stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        for tok in tokens:\n",
    "            clean_tok = stemmer.stem(tok).strip()\n",
    "            clean_tokens.append(clean_tok)\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for tok in tokens:\n",
    "            clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02f6566-c584-4427-960f-f65efb5ede96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7231801d-05c0-4918-b01f-471298204551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    # read in file\n",
    "    engine = create_engine(f'sqlite:///{data_file}.db')\n",
    "    df = pd.read_sql(f'select * from {data_file}', con=engine)\n",
    "\n",
    "    # define features and label arrays\n",
    "    X = df.message\n",
    "    Y = df.loc[:, 'related':]\n",
    "    target_names = Y.columns\n",
    "    y = Y.to_numpy()\n",
    "\n",
    "    return X, y, target_names\n",
    "\n",
    "X, y, target_names = load_data('disaster_response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9c7bb6-d42a-452c-94ca-e34c9a9a074c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dcb0f4ef-0c06-4710-8c87-507bb70f83ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 1.49 ms, total: 11.9 ms\n",
      "Wall time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = model.predict(['I need food', 'I am hungry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "090162fe-e137-4b1a-b4da-912712566388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        1      0            1             0                 0   \n",
       "1        1        1      0            1             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
       "0                  0         0         0            0  ...            0   \n",
       "1                  0         0         0            0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              1  \n",
       "1     0              0              1  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=target_names, data=pred)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2680ca58-95a1-49aa-8e4d-aace24f1c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   food\n",
       "0     1\n",
       "1     1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['food']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb3905eb-04d4-4a3c-83fa-e1d51c4b4d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Weather update - a cold front from Cuba that c...\n",
       "1                  Is the Hurricane over or is it not over\n",
       "2                          Looking for someone but no name\n",
       "3        UN reports Leogane 80-90 destroyed. Only Hospi...\n",
       "4        says: west side of Haiti, rest of the country ...\n",
       "                               ...                        \n",
       "26211    The training demonstrated how to enhance micro...\n",
       "26212    A suitable candidate has been selected and OCH...\n",
       "26213    Proshika, operating in Cox's Bazar municipalit...\n",
       "26214    Some 2,000 women protesting against the conduc...\n",
       "26215    A radical shift in thinking came about as a re...\n",
       "Name: message, Length: 26216, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47af7d5f-d136-448b-9d67-e25cd90c89cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Weather' in X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b255074e-12bc-4891-bae4-4b9879a51b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'disaster_response'\n",
    "engine = create_engine(f'sqlite:///{data_file}.db')\n",
    "df = pd.read_sql(f'select * from {data_file}', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ca49153-3c2c-411c-b484-a902bf0802ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7        Please, we need tents and water. We are in Sil...\n",
       "10       There's nothing to eat and water, we starving ...\n",
       "12       I am in Thomassin number 32, in the area named...\n",
       "16       We need food and water in Klecin 12. We are dy...\n",
       "22       There's a lack of water in Moleya, please info...\n",
       "                               ...                        \n",
       "26127    Officials have opened a barrage on the river n...\n",
       "26144    They will include basic cooking supplies, plat...\n",
       "26165    One of the first organizations to respond to t...\n",
       "26168    * Sanitation, (the importance of clean drinkin...\n",
       "26197    We're providing clean water to people who woul...\n",
       "Name: message, Length: 2581, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.message.str.contains(r'water')].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "70c0bea9-76d3-4d8d-95e4-b63439046949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2420"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.message.str.contains(r'food')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ed948f1-324b-4839-be8a-f8a37bb1d2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.message.str.contains(r'hungry')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "833c9396-0618-4c70-a474-9c3a105cc262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(, len(df[df.message.str.contains(r'food')].index.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b3a7d60-0cec-4c9b-a562-61df91456211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   16,    23,   118,   119,   130,   151,   174,   196,   233,\n",
       "              391,\n",
       "            ...\n",
       "            15702, 17445, 18272, 19140, 22612, 23443, 23666, 23834, 24645,\n",
       "            25818],\n",
       "           dtype='int64', length=167)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.message.str.contains(r'hungry')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16103157-8e81-4b59-b138-b90ed357d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = df[df.message.str.contains(r'food')].index.to_list()\n",
    "l2 = df[df.message.str.contains(r'hungry')].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9884194f-eac5-43cb-92e0-0c70e5699f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   16,    23,   119,   196,   862,  1040,  1464,  1480,  1576,\n",
       "        1677,  1684,  1769,  1873,  2070,  2549,  2576,  2790,  2812,\n",
       "        2819,  2831,  2872,  2907,  2947,  3069,  3079,  3749,  3863,\n",
       "        3889,  4007,  4374,  4941,  5411,  5413,  5428,  6496,  6708,\n",
       "        6759,  6785,  7078,  7160,  7749,  7937,  8230,  9889, 19140,\n",
       "       22612])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7e16e5f-2bf0-4c6b-8a9d-416a31853093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "                9,\n",
       "            ...\n",
       "            26206, 26207, 26208, 26209, 26210, 26211, 26212, 26213, 26214,\n",
       "            26215],\n",
       "           dtype='int64', length=25637)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.message.str.contains(r'[(hungry)(food)]')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "94893cbf-6d1c-4c9d-aca9-af53a21740df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26216"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "879bd81f-58bc-46e5-8618-bcf490fe0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f92cb30-4883-4fb1-93a0-545928cd1b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minfo_or_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'[nltk_data] '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhalt_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprint_error_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mipykernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miostream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutStream\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x10bc318e0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/site-packages/nltk/downloader.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk.download?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2b71ac4-c57e-4c35-92dd-c2def5e3bd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/Users/zacks/nltk_data/tokenizers/punkt/PY3')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24a29572-7431-44ff-848d-92a5f222af47",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mstem/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/zacks/nltk_data'\n    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bn/xnw38wz5697c2m0cykr9528m0000gn/T/ipykernel_60349/270596901.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stem/wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mstem/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/zacks/nltk_data'\n    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.data.find('stem/wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de8437b8-8490-49ef-9577-8ac289ddace5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Find the given resource by searching through the directories and\n",
       "zip files in paths, where a None or empty string specifies an absolute path.\n",
       "Returns a corresponding path name.  If the given resource is not\n",
       "found, raise a ``LookupError``, whose message gives a pointer to\n",
       "the installation instructions for the NLTK downloader.\n",
       "\n",
       "Zip File Handling:\n",
       "\n",
       "  - If ``resource_name`` contains a component with a ``.zip``\n",
       "    extension, then it is assumed to be a zipfile; and the\n",
       "    remaining path components are used to look inside the zipfile.\n",
       "\n",
       "  - If any element of ``nltk.data.path`` has a ``.zip`` extension,\n",
       "    then it is assumed to be a zipfile.\n",
       "\n",
       "  - If a given resource name that does not contain any zipfile\n",
       "    component is not found initially, then ``find()`` will make a\n",
       "    second attempt to find that resource, by replacing each\n",
       "    component *p* in the path with *p.zip/p*.  For example, this\n",
       "    allows ``find()`` to map the resource name\n",
       "    ``corpora/chat80/cities.pl`` to a zip file path pointer to\n",
       "    ``corpora/chat80.zip/chat80/cities.pl``.\n",
       "\n",
       "  - When using ``find()`` to locate a directory contained in a\n",
       "    zipfile, the resource name must end with the forward slash\n",
       "    character.  Otherwise, ``find()`` will not locate the\n",
       "    directory.\n",
       "\n",
       ":type resource_name: str or unicode\n",
       ":param resource_name: The name of the resource to search for.\n",
       "    Resource names are posix-style relative path names, such as\n",
       "    ``corpora/brown``.  Directory names will be\n",
       "    automatically converted to a platform-appropriate path separator.\n",
       ":rtype: str\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/site-packages/nltk/data.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk.data.find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff7770-8e0c-436b-8e69-288f3100b80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
