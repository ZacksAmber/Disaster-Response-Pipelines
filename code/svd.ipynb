{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f748d2-967e-48d6-ae3e-840dbdf1e830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4b06f3-a80d-4d47-942c-ad429b4af74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster_response.db')\n",
    "df = pd.read_sql('select * from disaster_response', con=engine)\n",
    "X = df.message\n",
    "Y = df.loc[:, 'related':]\n",
    "target_names = Y.columns\n",
    "y = Y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f5e0a8-7522-45cf-b249-fac3f4f89363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message, stem='lemm'):\n",
    "    \"\"\"Text processing.\n",
    "    \n",
    "    Args:\n",
    "        stem(str): stem or lemm.\n",
    "        \n",
    "    Returns:\n",
    "        list: Cleaned tokens.\n",
    "    \"\"\"\n",
    "    # 1. Cleaning\n",
    "    \n",
    "    # 2. Normalization\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", message.lower())\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Stop Word Removal\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tokens = list(filter(lambda w: w not in stop_words, tokens))\n",
    "    \n",
    "    # 5. Part of Speech Tagging / Named Entity Recognition\n",
    "    \n",
    "    # 6. Stemming or Lemmatization\n",
    "    # Because the targets are not roots, we should use Lemmatization\n",
    "    \n",
    "    clean_tokens = []\n",
    "    if stem == 'stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        for tok in tokens:\n",
    "            clean_tok = stemmer.stem(tok).strip()\n",
    "            clean_tokens.append(clean_tok)\n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for tok in tokens:\n",
    "            clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084dabf4-fec4-4654-834d-6d9a8fa58a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4a8de5-7bab-4a4b-9f5b-2ed2e7d55e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.9 s, sys: 613 ms, total: 8.51 s\n",
      "Wall time: 8.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "X_train_tfidf = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fafc6b3f-1475-4d59-9ae2-d499ab4e7458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972, 28191)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe07200-59ff-4ee9-8e2a-2356bc46abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 6.61 s, total: 1min 15s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)\n",
    "sla = svd.fit_transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5edf42c-f719-4a1c-9ab3-23b9fafa213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3672f900-d9bd-483c-bc14-844051df9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "multi_label_clf = MultiOutputClassifier(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b6012c-2e61-4859-beae-3a660bf846f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 3.15 s, total: 14.1 s\n",
      "Wall time: 11min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=-1,\n",
       "                                                       random_state=42))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "multi_label_clf.fit(sla, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e4bb0b-b232-4342-9b40-6b2faf6f417a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (3438313781.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/bn/xnw38wz5697c2m0cykr9528m0000gn/T/ipykernel_90851/3438313781.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "multi_label_clf.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b9b0d-580c-4e3a-b481-7b4d8211596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('svd', svd)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e9bb8-2813-41d8-b06c-3b347574130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743f64c-af81-4548-a275-9f69f1aecf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network needs much more time for modeling\n",
    "# mlp = MLPClassifier(random_state=42, max_iter=200, verbose=True, early_stopping=True)\n",
    "vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "forest = RandomForestClassifier(random_state=42)\n",
    "multi_label_clf = MultiOutputClassifier(forest)\n",
    "svd = TruncatedSVD(n_components=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c9d6be-5a35-4267-ada3-c4222d3ab8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('svd', svd),\n",
    "    ('multi_label_clf', multi_label_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079c3c62-4200-4c11-a4e7-4fd0d7130237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   TfidfVectorizer(tokenizer=<function tokenize at 0x1349ab790>)),\n",
       "  ('svd', TruncatedSVD(n_components=1000, random_state=42)),\n",
       "  ('multi_label_clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=-1,\n",
       "                                                          random_state=42)))],\n",
       " 'verbose': False,\n",
       " 'vect': TfidfVectorizer(tokenizer=<function tokenize at 0x1349ab790>),\n",
       " 'svd': TruncatedSVD(n_components=1000, random_state=42),\n",
       " 'multi_label_clf': MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=-1,\n",
       "                                                        random_state=42)),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.float64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__smooth_idf': True,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__sublinear_tf': False,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(message, stem='lemm')>,\n",
       " 'vect__use_idf': True,\n",
       " 'vect__vocabulary': None,\n",
       " 'svd__algorithm': 'randomized',\n",
       " 'svd__n_components': 1000,\n",
       " 'svd__n_iter': 5,\n",
       " 'svd__random_state': 42,\n",
       " 'svd__tol': 0.0,\n",
       " 'multi_label_clf__estimator__bootstrap': True,\n",
       " 'multi_label_clf__estimator__ccp_alpha': 0.0,\n",
       " 'multi_label_clf__estimator__class_weight': None,\n",
       " 'multi_label_clf__estimator__criterion': 'gini',\n",
       " 'multi_label_clf__estimator__max_depth': None,\n",
       " 'multi_label_clf__estimator__max_features': 'auto',\n",
       " 'multi_label_clf__estimator__max_leaf_nodes': None,\n",
       " 'multi_label_clf__estimator__max_samples': None,\n",
       " 'multi_label_clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'multi_label_clf__estimator__min_samples_leaf': 1,\n",
       " 'multi_label_clf__estimator__min_samples_split': 2,\n",
       " 'multi_label_clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'multi_label_clf__estimator__n_estimators': 100,\n",
       " 'multi_label_clf__estimator__n_jobs': -1,\n",
       " 'multi_label_clf__estimator__oob_score': False,\n",
       " 'multi_label_clf__estimator__random_state': 42,\n",
       " 'multi_label_clf__estimator__verbose': 0,\n",
       " 'multi_label_clf__estimator__warm_start': False,\n",
       " 'multi_label_clf__estimator': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       " 'multi_label_clf__n_jobs': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94335d21-9e07-4d42-97eb-983c8b8eba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
