{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401d0704-0c7f-4d9e-bf32-a25dd492a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zacks/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import joblib\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065ea747-c195-4615-b582-874990c93f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    # read in file\n",
    "    engine = create_engine(f'sqlite:///{data_file}.db')\n",
    "    df = pd.read_sql(f'select * from {data_file}', con=engine)\n",
    "\n",
    "    # define features and label arrays\n",
    "    X = df.message\n",
    "    Y = df.loc[:, 'related':]\n",
    "    target_names = Y.columns\n",
    "    y = Y.to_numpy()\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9fc9ff-b97f-4b4f-8906-4a27425e56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message, stem='lemm'):\n",
    "    \"\"\"Text processing.\n",
    "\n",
    "    Args:\n",
    "        message(str): Message content.\n",
    "        stem(str): stem or lemm.\n",
    "\n",
    "    Returns:\n",
    "        list: Cleaned tokens.\n",
    "    \"\"\"\n",
    "    # Cleaning\n",
    "    # replace each url in text string with placeholder\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    url_pattern = re.compile(url_regex)\n",
    "    message = url_pattern.sub('urlplaceholder', message)\n",
    "\n",
    "    # Normalization\n",
    "    message = re.sub(r\"[^a-zA-Z0-9]\", \" \", message.lower())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(message)\n",
    "\n",
    "    # Stop Word Removal & Stemming/Lemmatization\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    # because the targets are not roots, we should use Lemmatization\n",
    "    if stem == 'stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(tok)\n",
    "                for tok in tokens if tok not in stop_words] \n",
    "    else:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(tok, pos='v')\n",
    "                  for tok in tokens if tok not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44b8ac0-4684-4718-bc3a-d9f9421faa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_pipeline():\n",
    "    # text processing and model pipeline\n",
    "    vect = TfidfVectorizer(tokenizer=tokenize, use_idf=True, smooth_idf=True)\n",
    "    svd = TruncatedSVD(random_state=42)\n",
    "    # forest = RandomForestClassifier(random_state=42, n_jobs=16)\n",
    "    # mlp = MLPClassifier(random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "    knn = KNeighborsClassifier(n_jobs=16)\n",
    "    \n",
    "    multi_clf = MultiOutputClassifier(knn)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vect),\n",
    "        # ('svd', svd),\n",
    "        ('multi_clf', multi_clf)\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961b20f8-7b0f-4f22-bb53-6635064d68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0982f970-4a06-404b-8178-91dbcb230a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Modeling tuning with Target encoding.\n",
    "    \"\"\"\n",
    "    X, y = load_data('disaster_response')\n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    pipeline = ml_pipeline()\n",
    "\n",
    "    params = {\n",
    "        'vect__max_features': trial.suggest_int('vect__max_features', 100, 5000, 100),\n",
    "        'multi_clf__estimator__leaf_size': trial.suggest_int('multi_clf__estimator__leaf_size', 30, 50, 1),\n",
    "        'multi_clf__estimator__n_neighbors': trial.suggest_int('multi_clf__estimator__n_neighbors', 36, 50, 1)\n",
    "    }\n",
    "    \n",
    "    model = pipeline.set_params(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        scores.append(recall_score(y_test[:, i], y_pred[:, i], average='weighted'))\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c683a08-2eee-400f-b4b6-519358d347e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of trails\n",
    "n_trials = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e947d3-0533-4d62-8201-0c3ca73dba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2557f6813f6443f3ab64bf712b86d68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8h 48min 40s, sys: 1h 47min 9s, total: 10h 35min 50s\n",
      "Wall time: 4h 23min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize', study_name=f'NLP {n_trials} trails')\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True) # set n_triasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e02e03-c971-4c73-bc83-ee99b5a79df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__max_features': 100,\n",
       " 'multi_clf__estimator__leaf_size': 45,\n",
       " 'multi_clf__estimator__n_neighbors': 41}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c2afa0-11fa-40fb-b5a6-7876fdf25738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385700059327063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a97e7eb-4ff5-457f-9383-db7f5d68bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store study model\n",
    "# joblib.dump(study, \"study.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c731b24-7a7f-449a-91e9-10194c495a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
